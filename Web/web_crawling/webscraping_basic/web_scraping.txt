[Web scraping & web crolling]

web scraping = 웹 페이지에서 원하는 부분만 scraping

web crolling = 웹 페이지의 허용범위 내, link들을 따라가며 모든 내용들을 가져오기

[web]

HTML + CSS + JS

집 = WEB
- outline (structure) : HTML - 뼈대 - extension "open in browser"
- Interior(allignment) : CSS - 예쁘게
- Actual Implement : JAVA SCRIPT - 살아있게

*현재 시점의 web page에 대한 정보가 다르기 때문에 ,강의 내용 그대로 따라가기 보다는 현재 시점을 고려하면서 공부할 것.

##########################################################################################################################
[웹 스크래핑 변경사항]
1. "티스토리"는 UserAgent 를 변경하지 않아도 정상적으로 html 을 받아옵니다.

2. "네이버"는 로그인 시도 시 자동입력방지 문자 입력 페이지가 뜹니다. 우회방법으로 자바스크립트를 이용하는 방법이 소개된 링크를 참고해주세요.
https://jaeseokim.github.io/Python/python-Selenium을-이용한-웹-크롤링-Naver-login-후-구독-Feed-크롤링/

3. "쿠팡" 강의 내용 확인 결과 일부 항목이 웹에서 접근했을 때와는 조금 다르게 가져오는듯 합니다. 확인 결과 화면 중 약 80% 는 정상, 20%는 페이지에 존재하지 않는 값을 가져옵니다. (어쩌면 다음 페이지에 나오는 내용일 수도 있습니다) 또한 80% 의 항목도 웹 페이지와는 달리 순서가 조금 뒤죽박죽 섞인듯 보입니다. requests  만 써서 가져왔을 때 쿠팡에서 반환해주는 값에 차이가 있는듯한데,  selenium 을 통한 결과를 비교해볼 필요가 있어 보이네요. 수업 시간에 결과 내용에 대해 전수 검사를 해볼 생각을 미처 해보지 못하여 내용에 오류가 있었던 점, 진심으로 사과 드립니다.

4. "프로젝트" 강의 내용 중 네이버 뉴스를 가져올 때 500 Server Error 가 나고 있습니다. 이 때는 requests 에 headers 로 여러분 PC 의 user-agent 를 넣어주시면 됩니다.
(예시)
def create_soup(url):
    headers = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36"}

    res = requests.get(url, headers=headers)
    res.raise_for_status()    

    soup = BeautifulSoup(res.text, "lxml")
    return soup


-----------------------------------------------------------------------------
